# ğŸ—ï¸ Architecture Documentation - NASA Space Monitoring Platform

## Table of Contents
1. [System Architecture Overview](#system-architecture-overview)
2. [Data Architecture](#data-architecture)
3. [Component Details](#component-details)
4. [Data Flow](#data-flow)
5. [Technology Stack](#technology-stack)
6. [Scalability Considerations](#scalability-considerations)

---

## System Architecture Overview

The NASA Space Monitoring Platform follows a **modern data lakehouse architecture** with distinct layers for ingestion, processing, storage, and visualization.

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         PRESENTATION LAYER                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Streamlit Dashboard  â”‚  Jupyter Notebooks  â”‚  API Endpoints    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                        â”‚                â”‚
             â–¼                        â–¼                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ANALYTICS LAYER                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Aggregations  â”‚  Metrics  â”‚  Reports  â”‚  Alerts  â”‚  ML Models â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DATA WAREHOUSE (GOLD)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  DuckDB/PostgreSQL  â”‚  Star Schema  â”‚  Fact & Dimension Tables â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TRANSFORMATION LAYER (SILVER)                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Data Cleaning  â”‚  Validation  â”‚  Enrichment  â”‚  Quality Checksâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DATA LAKE (BRONZE)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Raw JSON  â”‚  Raw Parquet  â”‚  Unprocessed Data  â”‚  Backups    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        INGESTION LAYER                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  API Clients  â”‚  Rate Limiting  â”‚  Error Handling  â”‚  Logging  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DATA SOURCES                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  NeoWs API  â”‚  APOD  â”‚  Mars Rover  â”‚  DONKI  â”‚  Earth Imageryâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Orchestration Layer

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      APACHE AIRFLOW                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  Ingest  â”‚â†’ â”‚Transform â”‚â†’ â”‚   Load   â”‚â†’ â”‚Analytics â”‚       â”‚
â”‚  â”‚   DAG    â”‚  â”‚   DAG    â”‚  â”‚   DAG    â”‚  â”‚   DAG    â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚       â†“              â†“              â†“              â†“           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚           Task Scheduler & Executor                  â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚       â†“              â†“              â†“              â†“           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  Worker  â”‚  â”‚  Worker  â”‚  â”‚  Worker  â”‚  â”‚  Worker  â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Data Architecture

### Medallion Architecture (Bronze â†’ Silver â†’ Gold)

#### ğŸ¥‰ Bronze Layer (Raw Data)
- **Purpose**: Store raw, unprocessed data exactly as received from APIs
- **Format**: JSON, Parquet
- **Location**: `data/raw/`
- **Retention**: 90 days
- **Characteristics**:
  - Immutable
  - Schema-on-read
  - Complete history
  - No transformations

#### ğŸ¥ˆ Silver Layer (Processed Data)
- **Purpose**: Cleaned, validated, and enriched data
- **Format**: Parquet
- **Location**: `data/processed/`
- **Retention**: 365 days
- **Transformations**:
  - Data cleaning
  - Type conversions
  - Deduplication
  - Validation
  - Enrichment (calculated fields)

#### ğŸ¥‡ Gold Layer (Analytics-Ready Data)
- **Purpose**: Aggregated, business-ready data
- **Format**: DuckDB tables, Parquet
- **Location**: `data/analytics/`, `data/warehouse/`
- **Retention**: Indefinite
- **Characteristics**:
  - Star schema
  - Pre-aggregated
  - Optimized for queries
  - Business logic applied

### Star Schema Design

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   dim_asteroids  â”‚
                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                    â”‚ asteroid_id (PK) â”‚
                    â”‚ name             â”‚
                    â”‚ diameter_min_km  â”‚
                    â”‚ diameter_max_km  â”‚
                    â”‚ size_category    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    dim_time     â”‚          â”‚          â”‚  dim_hazard_     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤          â”‚          â”‚  classification  â”‚
â”‚ date_id (PK)    â”‚          â”‚          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ date            â”‚          â”‚          â”‚ hazard_id (PK)   â”‚
â”‚ year            â”‚          â”‚          â”‚ threat_level     â”‚
â”‚ month           â”‚          â”‚          â”‚ is_hazardous     â”‚
â”‚ day             â”‚          â–¼          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ day_of_week     â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ week_of_year    â”‚â—„â”€â”€â”¤ fact_asteroid_      â”‚â—„â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚    approaches       â”‚
                      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                      â”‚ approach_id (PK)    â”‚
                      â”‚ asteroid_id (FK)    â”‚
                      â”‚ date_id (FK)        â”‚
                      â”‚ hazard_id (FK)      â”‚
                      â”‚ miss_distance_km    â”‚
                      â”‚ miss_distance_lunar â”‚
                      â”‚ velocity_kms        â”‚
                      â”‚ risk_score          â”‚
                      â”‚ ingestion_timestamp â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Component Details

### 1. Ingestion Layer

**Components:**
- `base_client.py`: Base API client with retry logic, rate limiting
- `neo_ingestion.py`: NEO data collection
- `apod_ingestion.py`: Astronomy Picture of the Day
- `mars_ingestion.py`: Mars Rover photos
- `donki_ingestion.py`: Space weather events

**Features:**
- âœ… Automatic retry on failure (3 attempts)
- âœ… Exponential backoff
- âœ… Rate limiting (1000 req/hour)
- âœ… Request caching
- âœ… Comprehensive logging
- âœ… Error handling

**Data Sources:**
```python
NASA_APIS = {
    'NeoWs': 'https://api.nasa.gov/neo/rest/v1/',
    'APOD': 'https://api.nasa.gov/planetary/apod',
    'Mars Rover': 'https://api.nasa.gov/mars-photos/api/v1/',
    'DONKI': 'https://api.nasa.gov/DONKI/',
    'Earth': 'https://api.nasa.gov/planetary/earth/'
}
```

### 2. Transformation Layer

**Components:**
- `neo_transformer.py`: Clean and enrich NEO data
- Data validation with Pandera schemas
- Quality checks with Great Expectations
- Calculated metrics and enrichments

**Transformations:**
```python
Enrichments = [
    'diameter_avg_km',          # Average diameter
    'estimated_volume_km3',     # Volume estimate
    'days_until_approach',      # Time to approach
    'size_category',            # Small/Medium/Large/VeryLarge
    'risk_score',               # 0-100 risk metric
    'threat_level'              # High/Medium/Low/Minimal
]
```

### 3. Storage Layer

**DuckDB (Default):**
- Embedded OLAP database
- Fast analytical queries
- No server required
- Perfect for local development

**PostgreSQL (Production):**
- Used by Airflow for metadata
- Optional for data warehouse
- ACID compliance
- Better for concurrent users

### 4. Orchestration Layer

**Apache Airflow:**
- **Scheduler**: Triggers DAGs on schedule
- **Workers**: Execute tasks in parallel
- **Webserver**: UI for monitoring
- **Executor**: LocalExecutor or CeleryExecutor

**DAG Configuration:**
```python
DAG_SCHEDULE = {
    'neo_feed': '0 */6 * * *',      # Every 6 hours
    'apod': '0 8 * * *',             # Daily at 8 AM
    'mars_rover': '0 10 * * *',      # Daily at 10 AM
    'donki_events': '0 */12 * * *',  # Every 12 hours
}
```

### 5. Visualization Layer

**Streamlit Dashboard:**
- Real-time data visualization
- Interactive filters
- Downloadable reports
- Responsive design

**Jupyter Notebooks:**
- Exploratory data analysis
- Ad-hoc queries
- Prototyping
- Documentation

---

## Data Flow

### End-to-End Pipeline

```
1. SCHEDULED TRIGGER (Airflow)
   â†“
2. API REQUEST (Python client)
   â†“
3. RAW DATA STORAGE (Bronze - JSON/Parquet)
   â†“
4. DATA CLEANING (Pandas/Polars)
   â†“
5. DATA VALIDATION (Pandera/Great Expectations)
   â†“
6. DATA ENRICHMENT (Calculated fields)
   â†“
7. PROCESSED DATA STORAGE (Silver - Parquet)
   â†“
8. SCHEMA MAPPING (To star schema)
   â†“
9. DATA WAREHOUSE LOAD (Gold - DuckDB)
   â†“
10. AGGREGATIONS (Analytics layer)
    â†“
11. VISUALIZATION (Streamlit)
```

### Sample Data Flow Timing

| Stage | Duration | Notes |
|-------|----------|-------|
| API Request | ~2-5 sec | Depends on NASA API response time |
| Raw Storage | ~1 sec | Write JSON/Parquet to disk |
| Transformation | ~5-10 sec | For 100-200 records |
| Validation | ~2-3 sec | Schema checks |
| Warehouse Load | ~3-5 sec | DuckDB insert |
| Dashboard Refresh | ~1-2 sec | Query + render |
| **Total Pipeline** | **~15-30 sec** | For typical batch |

---

## Technology Stack

### Core Technologies

| Layer | Technology | Purpose |
|-------|------------|---------|
| **Data Collection** | Python 3.9+ | API clients, scripting |
| **API Client** | Requests, HTTPX | HTTP requests |
| **Data Processing** | Pandas, Polars | Data manipulation |
| **Storage** | DuckDB, PostgreSQL | Data warehouse |
| **File Format** | Parquet, JSON | Efficient storage |
| **Orchestration** | Apache Airflow | Workflow management |
| **Visualization** | Streamlit, Plotly | Dashboards |
| **Validation** | Pandera, Great Expectations | Data quality |
| **Logging** | Loguru | Structured logging |
| **Containerization** | Docker, Docker Compose | Deployment |

### Why These Technologies?

**Python**: 
- Rich data engineering ecosystem
- Easy NASA API integration
- Extensive libraries

**DuckDB**:
- In-process OLAP database
- Fast analytical queries
- No server overhead
- Perfect for local development

**Parquet**:
- Columnar storage format
- Excellent compression
- Fast read performance
- Schema evolution support

**Airflow**:
- Industry standard for orchestration
- Rich UI for monitoring
- Extensive operator library
- Python-native DAG definition

**Streamlit**:
- Rapid dashboard development
- Python-native
- Auto-refresh capabilities
- Easy deployment

---

## Scalability Considerations

### Current Architecture (Development)

**Designed for:**
- Single machine deployment
- 1-10 GB data volume
- Hourly/daily updates
- 1-10 concurrent users

**Characteristics:**
- âœ… Simple deployment (Docker Compose)
- âœ… Low resource requirements
- âœ… Easy to understand and maintain
- âŒ Limited horizontal scalability
- âŒ Single point of failure

### Production Architecture (Future)

**For scaling to:**
- Distributed deployment
- 100+ GB data volume
- Real-time updates
- 100+ concurrent users

**Recommended Changes:**

1. **Data Storage:**
   ```
   DuckDB â†’ PostgreSQL/Snowflake/BigQuery
   Local Files â†’ S3/GCS/Azure Blob
   ```

2. **Processing:**
   ```
   Pandas â†’ Apache Spark/Dask
   Single machine â†’ Distributed cluster
   ```

3. **Orchestration:**
   ```
   LocalExecutor â†’ KubernetesExecutor
   Docker Compose â†’ Kubernetes/ECS
   ```

4. **Caching:**
   ```
   Add Redis for query caching
   Add CDN for static assets
   ```

5. **Monitoring:**
   ```
   Add Prometheus + Grafana
   Add ELK stack for logs
   Add data lineage tracking
   ```

### Performance Optimization

**Query Optimization:**
```sql
-- Create indexes on frequently queried columns
CREATE INDEX idx_approach_date ON fact_asteroid_approaches(close_approach_date);
CREATE INDEX idx_hazardous ON fact_asteroid_approaches(is_potentially_hazardous);
CREATE INDEX idx_asteroid_id ON fact_asteroid_approaches(asteroid_id);

-- Partition large tables by date
CREATE TABLE fact_asteroid_approaches_partitioned (
    ...
) PARTITION BY RANGE (close_approach_date);
```

**Data Compression:**
```python
# Use appropriate Parquet compression
df.to_parquet(
    'data.parquet',
    compression='snappy',  # Fast compression
    # or 'gzip' for better compression ratio
)
```

---

## Security Considerations

### API Key Management
- âœ… Environment variables (.env)
- âœ… Never commit to Git
- âœ… Rotate keys periodically

### Database Security
- âœ… Strong passwords
- âœ… Network isolation
- âœ… Encryption at rest (production)
- âœ… SSL/TLS connections

### Access Control
- âœ… Airflow RBAC
- âœ… Database user permissions
- âœ… Dashboard authentication (production)

---

## Disaster Recovery

### Backup Strategy

**Bronze Layer (Raw Data):**
- Retention: 90 days
- Backup: Not needed (can re-fetch from API)

**Silver Layer (Processed Data):**
- Retention: 365 days
- Backup: Daily to S3/GCS

**Gold Layer (Warehouse):**
- Retention: Indefinite
- Backup: Daily snapshots
- Point-in-time recovery

### Recovery Procedures

**Scenario 1: Lost processed data**
```bash
# Re-transform from raw data
python src/transformation/neo_transformer.py
```

**Scenario 2: Lost raw data**
```bash
# Re-fetch from NASA API
python src/ingestion/neo_ingestion.py --backfill --days=90
```

**Scenario 3: Database corruption**
```bash
# Restore from backup
pg_restore -d nasa_space backup.dump
```

---

## Monitoring & Alerting

### Key Metrics to Monitor

1. **Data Quality:**
   - Record count per batch
   - Null value percentage
   - Schema validation failures
   - Duplicate records

2. **Performance:**
   - Pipeline execution time
   - API response time
   - Query latency
   - Resource utilization

3. **Reliability:**
   - DAG success rate
   - Task failure count
   - API rate limit hits
   - Data freshness

### Alert Thresholds

```python
ALERT_THRESHOLDS = {
    'pipeline_duration': 3600,  # 1 hour max
    'api_errors': 5,  # consecutive
    'null_percentage': 10,  # percent
    'data_staleness': 86400,  # 24 hours
}
```

---

**For questions or contributions, see the main README.md**
